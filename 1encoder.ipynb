{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.8509,  0.3531, -0.2358,  ...,  1.7505,  2.0493, -0.4218],\n",
       "         [-2.1914, -0.6323, -1.1931,  ...,  2.4355,  1.4947, -2.5901],\n",
       "         [-2.5607,  0.6912,  1.5422,  ...,  2.8686,  0.5209,  0.4583],\n",
       "         ...,\n",
       "         [-1.4648,  1.0369,  0.4630,  ...,  1.1369,  0.5880, -0.0726],\n",
       "         [-2.3465,  0.5403,  2.3221,  ...,  1.6129, -0.6126,  0.1816],\n",
       "         [-2.2795, -1.4391,  1.2479,  ...,  2.5200,  0.2735, -0.7442]],\n",
       "\n",
       "        [[-1.8509,  0.3531, -0.2358,  ...,  1.7505,  2.0493, -0.4218],\n",
       "         [-2.1914, -0.6323, -1.1931,  ...,  2.4355,  1.4947, -2.5901],\n",
       "         [-2.5607,  0.6912,  1.5422,  ...,  2.8686,  0.5209,  0.4583],\n",
       "         ...,\n",
       "         [-1.4648,  1.0369,  0.4630,  ...,  1.1369,  0.5880, -0.0726],\n",
       "         [-2.3465,  0.5403,  2.3221,  ...,  1.6129, -0.6126,  0.1816],\n",
       "         [-2.2795, -1.4391,  1.2479,  ...,  2.5200,  0.2735, -0.7442]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class Embed(torch.nn.Module):\n",
    "    \"\"\"This part is the embedding part of the model.\n",
    "\n",
    "    Args:\n",
    "        torch (_type_): 输入的是一个tensor，shape为[b, 77]，其中b为batch_size，77为每个句子的长度，即77个token。\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Embed, self).__init__()\n",
    "        self.embed = torch.nn.Embedding(49408, 768)\n",
    "        self.pos_embed = torch.nn.Embedding(77, 768)\n",
    "\n",
    "        self.register_buffer(\"pos_ids\", torch.arange(77).unsqueeze(dim=0))\n",
    "        \n",
    "    def forward(self, input_ids):\n",
    "        # input_ids = [b, 77]\n",
    "        \n",
    "        # [b ,77] -> [b, 77, 768]\n",
    "        embed = self.embed(input_ids)\n",
    "        \n",
    "        # [1, 77] -> [1, 77, 768]\n",
    "        pos_embed = self.pos_embed(self.pos_ids)\n",
    "        \n",
    "        return embed + pos_embed  # 这里面有一个broadcast机制，pos_embed会自动broadcast成[b, 77, 768]的形状\n",
    "    \n",
    "Embed()(torch.ones(2,77).long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 77, 768])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from turtle import forward\n",
    "\n",
    "\n",
    "class Attention(torch.nn.Module):\n",
    "    \"\"\"注意力机制\n",
    "\n",
    "    Args:\n",
    "        torch (_type_): _description_\n",
    "    \"\"\"\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.q = torch.nn.Linear(768, 768)\n",
    "        self.k = torch.nn.Linear(768, 768)\n",
    "        self.v = torch.nn.Linear(768, 768)\n",
    "        self.out = torch.nn.Linear(768, 768)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x -> [b, 77, 768]\n",
    "        b = x.shape[0]\n",
    "        \n",
    "        # 纬度不变\n",
    "        q = self.q(x)\n",
    "        k = self.k(x)\n",
    "        v = self.v(x)\n",
    "        \n",
    "        # 拆分注意力头,拆分成12个\n",
    "        q = q.reshape(b, 77, 12, 64).transpose(1,2).reshape(b*12, 77, 64) * 0.125\n",
    "        k = k.reshape(b, 77, 12, 64).transpose(1,2).reshape(b*12, 77, 64)\n",
    "        v = v.reshape(b, 77, 12, 64).transpose(1,2).reshape(b*12, 77, 64)\n",
    "        \n",
    "        # 计算注意力得分\n",
    "        # [b*12, 77, 64] * [b*12, 64, 77] -> [b*12, 77, 77]\n",
    "        attn = torch.bmm(q, k.transpose(1, 2))\n",
    "        \n",
    "        # [b*12, 77, 77] -> [b,12, 77, 77]\n",
    "        attn = attn.reshape(b, 12, 77, 77)\n",
    "        \n",
    "        # 上三角掩码，b个词，\n",
    "        def  get_mask(b):\n",
    "            mask = torch.empty(b, 77, 77)\n",
    "            \n",
    "            # 上三角负无穷\n",
    "            mask.fill_(float(\"-inf\"))\n",
    "            \n",
    "            mask.triu_(1)\n",
    "            \n",
    "            return mask.unsqueeze(1)  # [b, 1, 77, 77]\n",
    "        \n",
    "        # [b, 12, 77, 77] + [b, 1, 77, 77] = [b, 12, 77, 77]\n",
    "        attn = attn + get_mask(attn.shape[0]).to(attn.device)\n",
    "        \n",
    "        # [b, 12, 77, 77] -> [b*12, 77, 77]\n",
    "        attn = attn.reshape(b*12, 77, 77)\n",
    "        \n",
    "        # sofrmax, 被mask的部分是0\n",
    "        attn = attn.softmax(dim = -1)\n",
    "        \n",
    "        # 和v的乘积\n",
    "        # [b*12, 77, 77] * [b*12, 77, 64] -> [b*12, 77, 64]\n",
    "        attn = torch.bmm(attn, v)\n",
    "        \n",
    "        # 恢复回去\n",
    "        attn = attn.reshape(b, 12, 77, 64).transpose(1,2).reshape(b, 77, 768)\n",
    "        \n",
    "        return self.out(attn)\n",
    "    \n",
    "Attention()(torch.ones(2, 77, 768)).shape\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 77, 768])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bert 的编码器层\n",
    "class ClipEncoder(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.s1 = torch.nn.Sequential(\n",
    "            torch.nn.LayerNorm(768),\n",
    "            Attention(),\n",
    "        )\n",
    "        \n",
    "        self.s2 = torch.nn.Sequential(\n",
    "            torch.nn.LayerNorm(768),\n",
    "            torch.nn.Linear(768, 3072),\n",
    "        )\n",
    "        \n",
    "        self.s3 = torch.nn.Linear(3072, 768)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x -> [b, 77, 768]\n",
    "        \n",
    "        # 维度不变\n",
    "        x = x +self.s1(x)\n",
    "        \n",
    "        # [2, 77, 768]\n",
    "        res = x\n",
    "\n",
    "        # [b, 77, 768] -> [b, 77, 3072]\n",
    "        x = self.s2(x)\n",
    "        \n",
    "        # 激活函数\n",
    "        x = x * (x * 1.702).sigmoid()\n",
    "        \n",
    "        return res + self.s3(x)\n",
    "    \n",
    "ClipEncoder()(torch.randn(2, 77, 768)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#经过优化之后的代码量少得吓人...\n",
    "encoder = torch.nn.Sequential(\n",
    "    Embed(),\n",
    "    ClipEncoder(),\n",
    "    ClipEncoder(),\n",
    "    ClipEncoder(),\n",
    "    ClipEncoder(),\n",
    "    ClipEncoder(),\n",
    "    ClipEncoder(),\n",
    "    ClipEncoder(),\n",
    "    ClipEncoder(),\n",
    "    ClipEncoder(),\n",
    "    ClipEncoder(),\n",
    "    ClipEncoder(),\n",
    "    ClipEncoder(),\n",
    "    torch.nn.LayerNorm(768),\n",
    ")\n",
    "\n",
    "# encoder(torch.ones(2, 77).long()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIPTextModel(\n",
      "  (text_model): CLIPTextTransformer(\n",
      "    (embeddings): CLIPTextEmbeddings(\n",
      "      (token_embedding): Embedding(49408, 768)\n",
      "      (position_embedding): Embedding(77, 768)\n",
      "    )\n",
      "    (encoder): CLIPEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0): CLIPEncoderLayer(\n",
      "          (self_attn): CLIPAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): CLIPMLP(\n",
      "            (activation_fn): QuickGELUActivation()\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (1): CLIPEncoderLayer(\n",
      "          (self_attn): CLIPAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): CLIPMLP(\n",
      "            (activation_fn): QuickGELUActivation()\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (2): CLIPEncoderLayer(\n",
      "          (self_attn): CLIPAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): CLIPMLP(\n",
      "            (activation_fn): QuickGELUActivation()\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (3): CLIPEncoderLayer(\n",
      "          (self_attn): CLIPAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): CLIPMLP(\n",
      "            (activation_fn): QuickGELUActivation()\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (4): CLIPEncoderLayer(\n",
      "          (self_attn): CLIPAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): CLIPMLP(\n",
      "            (activation_fn): QuickGELUActivation()\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (5): CLIPEncoderLayer(\n",
      "          (self_attn): CLIPAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): CLIPMLP(\n",
      "            (activation_fn): QuickGELUActivation()\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (6): CLIPEncoderLayer(\n",
      "          (self_attn): CLIPAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): CLIPMLP(\n",
      "            (activation_fn): QuickGELUActivation()\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (7): CLIPEncoderLayer(\n",
      "          (self_attn): CLIPAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): CLIPMLP(\n",
      "            (activation_fn): QuickGELUActivation()\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (8): CLIPEncoderLayer(\n",
      "          (self_attn): CLIPAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): CLIPMLP(\n",
      "            (activation_fn): QuickGELUActivation()\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (9): CLIPEncoderLayer(\n",
      "          (self_attn): CLIPAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): CLIPMLP(\n",
      "            (activation_fn): QuickGELUActivation()\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (10): CLIPEncoderLayer(\n",
      "          (self_attn): CLIPAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): CLIPMLP(\n",
      "            (activation_fn): QuickGELUActivation()\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (11): CLIPEncoderLayer(\n",
      "          (self_attn): CLIPAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): CLIPMLP(\n",
      "            (activation_fn): QuickGELUActivation()\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPTextModel\n",
    "\n",
    "local_model_path = 'E:\\Myproject\\\\02Model\\HuggingFace-Download-Accelerator\\hf_hub\\diffusion'\n",
    "# 加载预训练\n",
    "params = CLIPTextModel.from_pretrained(local_model_path)\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 词编码\n",
    "encoder[0].embed.load_state_dict(\n",
    "    params.text_model.embeddings.token_embedding.state_dict()\n",
    ")\n",
    "\n",
    "# 位置编码\n",
    "encoder[0].pos_embed.load_state_dict(\n",
    "    params.text_model.embeddings.position_embedding.state_dict()\n",
    ")\n",
    "\n",
    "# 12层编码层\n",
    "for i in range(12):\n",
    "    encoder[i+1].s1[0].load_state_dict(\n",
    "        params.text_model.encoder.layers[i].layer_norm1.state_dict()\n",
    "    )\n",
    "\n",
    "    #注意力q矩阵\n",
    "    encoder[i + 1].s1[1].q.load_state_dict(\n",
    "        params.text_model.encoder.layers[i].self_attn.q_proj.state_dict())\n",
    "\n",
    "    #注意力k矩阵\n",
    "    encoder[i + 1].s1[1].k.load_state_dict(\n",
    "        params.text_model.encoder.layers[i].self_attn.k_proj.state_dict())\n",
    "\n",
    "    #注意力v矩阵\n",
    "    encoder[i + 1].s1[1].v.load_state_dict(\n",
    "        params.text_model.encoder.layers[i].self_attn.v_proj.state_dict())\n",
    "\n",
    "    #注意力out\n",
    "    encoder[i + 1].s1[1].out.load_state_dict(\n",
    "        params.text_model.encoder.layers[i].self_attn.out_proj.state_dict())\n",
    "\n",
    "    #第二层norm\n",
    "    encoder[i + 1].s2[0].load_state_dict(\n",
    "        params.text_model.encoder.layers[i].layer_norm2.state_dict())\n",
    "\n",
    "    #mlp第一层fc\n",
    "    encoder[i + 1].s2[1].load_state_dict(\n",
    "        params.text_model.encoder.layers[i].mlp.fc1.state_dict())\n",
    "\n",
    "    #mlp第二层fc\n",
    "    encoder[i + 1].s3.load_state_dict(\n",
    "        params.text_model.encoder.layers[i].mlp.fc2.state_dict())\n",
    "\n",
    "# 输出norm\n",
    "encoder[13].load_state_dict(params.text_model.final_layer_norm.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.3488,  0.0139, -0.0409,  ..., -0.4707, -0.2910,  0.0627],\n",
      "         [ 0.6009, -0.4915,  1.0705,  ...,  0.0032,  0.5970, -0.4605],\n",
      "         [ 0.5848, -1.8402,  0.6390,  ...,  0.3736,  0.1611,  1.0529],\n",
      "         ...,\n",
      "         [ 0.7383, -0.1099,  1.2613,  ...,  0.2626, -0.2641,  0.3401],\n",
      "         [ 1.1845, -0.1865,  1.5217,  ...,  0.2758,  0.1133,  0.1809],\n",
      "         [ 0.9668, -0.5271,  1.4090,  ..., -0.0710,  0.1474, -0.2603]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[[-0.3488,  0.0139, -0.0409,  ..., -0.4707, -0.2910,  0.0627],\n",
      "         [ 0.6009, -0.4915,  1.0705,  ...,  0.0032,  0.5970, -0.4605],\n",
      "         [ 0.5848, -1.8402,  0.6390,  ...,  0.3736,  0.1611,  1.0529],\n",
      "         ...,\n",
      "         [ 0.7383, -0.1099,  1.2613,  ...,  0.2626, -0.2641,  0.3401],\n",
      "         [ 1.1845, -0.1865,  1.5217,  ...,  0.2758,  0.1133,  0.1809],\n",
      "         [ 0.9668, -0.5271,  1.4090,  ..., -0.0710,  0.1474, -0.2603]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = encoder(torch.arange(77).unsqueeze(dim=0))\n",
    "b = params(torch.arange(77).unsqueeze(dim=0)).last_hidden_state\n",
    "print(a)\n",
    "print(b)\n",
    "(a == b).all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
